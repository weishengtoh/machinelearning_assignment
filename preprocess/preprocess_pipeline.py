'''
Define the pipeline component that is used to clean the raw data used for training
and evaluation.

This pipeline component uses the raw data artifact generated by the dataloader_pipeline
component. The raw data artifact student-mat.csv is retrieved from Weights & Biases.

This pipeline component generates the cleaned data artifact as student-maths-clean.csv
which is stored locally in data/clean and in Weights & Biases

The artifact generated from this pipeline component is to be used by the 
test_data_pipeline component.
'''

import logging
import os

import hydra
import pandas as pd
import wandb
from omegaconf import DictConfig, OmegaConf
from sklearn.preprocessing import label_binarize

logging.basicConfig(level=logging.INFO, format="%(asctime)-15s %(message)s")
logger = logging.getLogger()


@hydra.main(config_path=os.path.join(os.pardir, 'configs'),
            config_name='preprocess_config')
def start_pipeline(preprocess_config: DictConfig):

    preprocess_config = OmegaConf.to_container(preprocess_config)
    params = preprocess_config['parameters']

    # Downloads the raw data artifact from W&B, and read it into a pandas Dataframe
    logger.info('Download artifact from W&B')
    run = wandb.init(project=preprocess_config['main']['project_name'],
                     group=preprocess_config['main']['experiment_name'],
                     job_type=preprocess_config['main']['job_type'])
    artifact = run.use_artifact(params['input_artifact'])
    artifact_dir = artifact.file()

    df = pd.read_csv(artifact_dir)

    # Bin the labels for binary classification
    y = pd.cut(df['G3'], bins=2, labels=['fail', 'pass'], ordered=False)
    df['G3'] = label_binarize(y, classes=['fail', 'pass']).ravel()

    # Rename the columns to increase comprehensibility
    df = rename_columns(df)

    # Drop all duplicated values
    df.drop_duplicates(inplace=True)

    # Define the paths required
    cwd_path = hydra.utils.get_original_cwd(
    )  # This is required to access the cwd when using Hydra
    root_path = os.path.join(cwd_path, os.pardir)
    output_folder = os.path.join(*[root_path, 'data', params['output_folder']])
    output_file = os.path.join(output_folder, params['artifact_name'])

    # Save the cleaned data in a local directory and upload to W&B
    os.makedirs(output_folder, exist_ok=True)
    df.to_csv(output_file, index=False)

    logger.info('Create and upload a W&B Artifact')
    artifact = wandb.Artifact(name=params['artifact_name'],
                              type=params['artifact_type'],
                              description=params['artifact_descript'])

    artifact.add_file(output_file)
    run.log_artifact(artifact)

    # Finish the wandb run
    wandb.finish()


def rename_columns(df: pd.DataFrame) -> pd.DataFrame:

    # Identify the original columns names
    nominal_var = [
        'school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',
        'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities',
        'nursery', 'higher', 'internet', 'romantic'
    ]
    numerical_var = ['age', 'absences']
    ordinal_var = [
        'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',
        'freetime', 'goout', 'Dalc', 'Walc', 'health'
    ]
    target_var = ['G1', 'G2', 'G3']

    # Define the new column names
    nominal_var_renamed = [
        'school', 'sex', 'address', 'fam_size', 'parents_cohabit', 'mother_job',
        'father_job', 'reason_for_school', 'guardian', 'extra_edu_support',
        'fam_edu_support', 'extra_paid_classes', 'extra_curri', 'att_nursery',
        'wants_higheredu', 'internet_access', 'in_romantic'
    ]
    numerical_var_renamed = ['age', 'num_school_absences']
    ordinal_var_renamed = [
        'mother_edu', 'father_edu', 'travel_time', 'weekly_studytime',
        'past_classfailures', 'famrelation_quality', 'afterschool_time',
        'friend_time', 'alc_consump_workday', 'alc_consump_weekend',
        'health_status'
    ]
    target_var_renamed = ['first_grade', 'second_grade', 'third_grade']

    nominal_var_dict = dict(zip(nominal_var, nominal_var_renamed))
    numerical_ver_dict = dict(zip(numerical_var, numerical_var_renamed))
    ordinal_var_dict = dict(zip(ordinal_var, ordinal_var_renamed))
    target_var_dict = dict(zip(target_var, target_var_renamed))
    renamed_dict = {
        **nominal_var_dict,
        **numerical_ver_dict,
        **ordinal_var_dict,
        **target_var_dict
    }

    return df.rename(columns=renamed_dict)


if __name__ == "__main__":

    start_pipeline()
